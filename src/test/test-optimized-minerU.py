import os
import time
import re
import json
import fitz
import gc
from pathlib import Path
from loguru import logger
from concurrent.futures import ProcessPoolExecutor

# ================= 1. ç¯å¢ƒé…ç½® =================
os.environ['MINERU_MODEL_SOURCE'] = "modelscope"
os.environ['MINERU_DEVICE_MODE'] = "cuda:0"
os.environ['MODELSCOPE_LOG_LEVEL'] = '40'

# é™é»˜åº•å±‚è¾“å‡º
fitz.TOOLS.mupdf_display_errors(False)

from mineru.cli.common import prepare_env
from mineru.data.data_reader_writer import FileBasedDataWriter
from mineru.utils.enum_class import MakeMode
from mineru.backend.pipeline.pipeline_analyze import doc_analyze as pipeline_doc_analyze
from mineru.backend.pipeline.pipeline_middle_json_mkcontent import union_make as pipeline_union_make
from mineru.backend.pipeline.model_json_to_middle_json import result_to_middle_json as pipeline_result_to_middle_json

# ================= 2. å‰ªææ ¸å¿ƒé€»è¾‘ (ç”± CPU è¿›ç¨‹æ± æ‰§è¡Œ) =================

def cpu_prune_worker(pdf_path):
    """
    è¯¥å‡½æ•°åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œï¼Œæ‰§è¡Œ CPU å¯†é›†çš„ PDF æ‰«æå’Œåˆ‡ç‰‡ä»»åŠ¡
    """
    try:
        re_method = re.compile(r'^\s*(?:2|3|II|III)\.?\s+(?:Method|System|Architecture|Design)', re.I | re.M)
        re_ref = re.compile(r'^\s*(?:References|REFERENCES|Bibliography)', re.I | re.M)
        re_visual = re.compile(r'\b(Table|Figure|Fig\.)\s+\d+\b', re.I)

        doc = fitz.open(pdf_path)
        total_pages = doc.page_count
        target_pages = {0, 1}
        idx_ref = -1
        page_texts = []

        for i, page in enumerate(doc):
            # æé€Ÿæå–æ–‡æœ¬å—
            blocks = page.get_text("blocks", sort=True)
            txt = "\n".join([b[4] for b in blocks if b[6] == 0])
            page_texts.append(txt)
            
            # å®šä½å…³é”®ç« èŠ‚å’Œå›¾è¡¨
            if idx_ref == -1 and i > total_pages * 0.5 and re_ref.search(txt):
                idx_ref = i
            if self_is_visual(page, txt, re_visual):
                if i > 1 and (idx_ref == -1 or i <= idx_ref):
                    target_pages.add(i)

        if idx_ref > 0: target_pages.add(idx_ref - 1)
        
        # æ‰§è¡Œåˆ‡ç‰‡
        target_indices = sorted(list(target_pages))
        new_doc = fitz.open()
        for p in target_indices:
            new_doc.insert_pdf(doc, from_page=p, to_page=p)
        
        pruned_bytes = new_doc.tobytes(garbage=3, deflate=True)
        doc.close()
        new_doc.close()
        
        return {
            "name": Path(pdf_path).stem,
            "bytes": pruned_bytes,
            "page_texts": page_texts,
            "idx_ref": idx_ref,
            "target_indices": target_indices,
            "total_pages": total_pages
        }
    except Exception as e:
        return {"error": str(e), "name": Path(pdf_path).stem}

def self_is_visual(page, txt, re_visual):
    if len(page.get_images()) > 0: return True
    if re_visual.search(txt): return True
    return False

# ================= 3. æ‰¹é‡å¤„ç†å¼•æ“ =================

class EdgeScholarBatchEngine:
    def __init__(self, output_root):
        self.output_root = output_root

    def run_benchmark(self, pdf_folder, batch_size=10):
        pdf_paths = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")][:batch_size]
        
        # --- Step 1: é¢„çƒ­ (Warm-up) ---
        logger.info("ğŸ”¥ æ­£åœ¨é¢„çƒ­æ¨¡å‹ (æ¶ˆé™¤åŠ è½½å¼€é”€)...")
        first_pdf_bytes = open(pdf_paths[0], "rb").read()
        # å¼ºåˆ¶åŠ è½½æƒé‡åˆ°æ˜¾å­˜
        _ = pipeline_doc_analyze([first_pdf_bytes], ['en'], formula_enable=False, table_enable=False)
        logger.info("âœ… é¢„çƒ­å®Œæˆã€‚")

        # --- Step 2: CPU å¹¶è¡Œå‰ªæ ---
        logger.info(f"âš™ï¸ æ­£åœ¨å¹¶è¡Œå‰ªæ {len(pdf_paths)} ç¯‡è®ºæ–‡...")
        t_cpu_start = time.perf_counter()
        
        with ProcessPoolExecutor(max_workers=min(len(pdf_paths), 10)) as executor:
            pruned_data_list = list(executor.map(cpu_prune_worker, pdf_paths))
        
        # è¿‡æ»¤æ‰å¤±è´¥çš„
        valid_data = [d for d in pruned_data_list if "error" not in d]
        cpu_duration = time.perf_counter() - t_cpu_start
        logger.info(f"âœ… CPU å‰ªæè€—æ—¶: {cpu_duration:.2f}s (å¹³å‡: {cpu_duration/len(valid_data):.2f}s/ç¯‡)")

        # --- Step 3: GPU æ‰¹é‡æ¨ç† (æ ¸å¿ƒååé‡æµ‹è¯•) ---
        logger.info(f"ğŸš€ å¼€å§‹ GPU æ‰¹é‡æ¨ç† (Batch Size: {len(valid_data)})...")
        t_gpu_start = time.perf_counter()
        
        batch_bytes = [d['bytes'] for d in valid_data]
        # è°ƒç”¨æ ¸å¿ƒåˆ†æ API
        infer_results, all_images, all_docs, langs, ocrs = pipeline_doc_analyze(
            batch_bytes, ['en'] * len(valid_data), 
            formula_enable=False, table_enable=True
        )
        
        gpu_duration = time.perf_counter() - t_gpu_start
        logger.info(f"âš¡ GPU æ‰¹é‡æ¨ç†å®Œæˆï¼è€—æ—¶: {gpu_duration:.2f}s (å¹³å‡: {gpu_duration/len(valid_data):.2f}s/ç¯‡)")

        # --- Step 4: ç»“æœä¿å­˜ ---
        logger.info("ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æ„åŒ– Markdown æŠ¥å‘Š...")
        for i, data in enumerate(valid_data):
            self.save_paper_result(data, infer_results[i], all_images[i], all_docs[i], langs[i], ocrs[i])

        total_time = cpu_duration + gpu_duration
        print("\n" + "="*50)
        print(f"ğŸ“Š æ‰¹å¤„ç†æ€§èƒ½æŠ¥å‘Š (n={len(valid_data)})")
        print("-" * 50)
        print(f"å¹³å‡ CPU å‰ªæè€—æ—¶:   {cpu_duration/len(valid_data):.4f}s")
        print(f"å¹³å‡ GPU æ¨ç†è€—æ—¶:   {gpu_duration/len(valid_data):.4f}s")
        print(f"å•ç¯‡å¹³å‡å¤„ç†é€Ÿåº¦:    {total_time/len(valid_data):.4f}s")
        print(f"ç³»ç»Ÿæ€»ååé‡:        {60 / (total_time/len(valid_data)):.2f} papers/min")
        print("="*50)

    def save_paper_result(self, data, res, imgs, doc, lang, ocr_en):
        name = data['name']
        try:
            local_image_dir, local_md_dir = prepare_env(self.output_root, name, "pipeline")
            image_writer = FileBasedDataWriter(local_image_dir)
            
            # ç»“æœè½¬æ¢å¹¶ä¿å­˜å›¾ç‰‡
            middle_json = pipeline_result_to_middle_json(
                res, imgs, doc, image_writer, lang, ocr_en, formula_enabled=False
            )
            
            # è·å–è§†è§‰å ä½ç¬¦
            visual_md = pipeline_union_make(middle_json["pdf_info"], MakeMode.MM_MD, str(Path(local_image_dir).name))
            
            # ä¿å­˜ Markdown
            report_md = f"# {name}\n\n[Pages Analyzed: {data['target_indices']}]\n\n"
            report_md += "---\n## Visual Evidence\n" + visual_md
            
            # --- å…³é”®ä¿®å¤ç‚¹ï¼šå¢åŠ  errors='replace' é˜²æ­¢ç¼–ç å´©æºƒ ---
            save_path = Path(local_md_dir) / f"{name}_report.md"
            with open(save_path, "w", encoding="utf-8", errors="replace") as f:
                f.write(report_md)
                
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœæ—¶å‡ºé”™ {name}: {e}")

if __name__ == "__main__":
    engine = EdgeScholarBatchEngine("./output_batch_test")
    # ä¸€æ¬¡æ€§è·‘ 10 ç¯‡
    engine.run_benchmark("./osdi2025", batch_size=10)