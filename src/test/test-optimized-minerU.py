import os
import time
import re
import json
import fitz
import gc
import pickle
from pathlib import Path
from loguru import logger
from concurrent.futures import ProcessPoolExecutor

# ================= 1. ç¯å¢ƒé…ç½® =================
os.environ['MINERU_MODEL_SOURCE'] = "local"
os.environ['MINERU_DEVICE_MODE'] = "cuda:0"
os.environ['MODELSCOPE_LOG_LEVEL'] = '40'

fitz.TOOLS.mupdf_display_errors(False)

from mineru.cli.common import prepare_env
from mineru.data.data_reader_writer import FileBasedDataWriter
from mineru.utils.enum_class import MakeMode
from mineru.backend.pipeline.pipeline_analyze import doc_analyze as pipeline_doc_analyze
from mineru.backend.pipeline.pipeline_middle_json_mkcontent import union_make as pipeline_union_make
from mineru.backend.pipeline.model_json_to_middle_json import result_to_middle_json as pipeline_result_to_middle_json

# ================= 2. é¢„å¤„ç† Worker (ä¿æŒä½ çš„é€»è¾‘) =================

def cpu_pre_process_worker(pdf_path):
    try:
        re_ref = re.compile(r'^\s*(?:References|REFERENCES|Bibliography)', re.I | re.M)
        re_visual = re.compile(r'\b(Table|Figure|Fig\.)\s+\d+\b', re.I)

        doc = fitz.open(pdf_path)
        total_pages = doc.page_count
        idx_ref = -1
        page_info = [] 
        ocr_indices = []
        
        for i, page in enumerate(doc):
            blocks = page.get_text("blocks", sort=True)
            txt = "\n".join([b[4] for b in blocks if b[6] == 0])
            
            if idx_ref == -1 and i > total_pages * 0.5 and re_ref.search(txt):
                idx_ref = i
            
            if self_is_visual(page, txt, re_visual):
                # éµå¾ªä½ çš„é€»è¾‘ï¼šä»…åœ¨å¼•ç”¨é¡µä¹‹å‰è¿›è¡Œ OCR
                if idx_ref == -1 or i <= idx_ref:
                    page_info.append({"type": "ocr", "page_idx": i})
                    ocr_indices.append(i)
            else:
                page_info.append({"type": "text", "content": txt, "page_idx": i})

        # ç”Ÿæˆå¾®å‹ PDF
        pruned_bytes = None
        if ocr_indices:
            new_doc = fitz.open()
            for p in ocr_indices:
                new_doc.insert_pdf(doc, from_page=p, to_page=p)        
            pruned_bytes = new_doc.tobytes(garbage=3, deflate=True)
            new_doc.close()
        
        doc.close()
        return {
            "name": Path(pdf_path).stem,
            "ocr_bytes": pruned_bytes,
            "ocr_mapping": ocr_indices,
            "page_structure": page_info,
            "status": "success"
        }
    except Exception as e:
        return {"status": "error", "error": str(e), "name": Path(pdf_path).stem}

def self_is_visual(page, txt, re_visual):
    if len(page.get_images()) > 0: return True
    if re_visual.search(txt): return True
    return False

# ================= 3. å¤šæ ¸ä¿å­˜ Worker (æ¥æ”¶åºåˆ—åŒ– dict) =================

def cpu_save_worker(data_pack):
    """
    è´Ÿè´£ï¼š1. æ–‡æœ¬ç²¾å‡†åˆ‡ç‰‡ 2. ç¼åˆè§†è§‰ç»„ä»¶ 3. å¼‚æ­¥å­˜ç›˜
    """
    (middle_json_dict, meta, output_root) = data_pack
    name = meta['name']
    try:
        # å‡†å¤‡è·¯å¾„
        local_image_dir, local_md_dir = prepare_env(output_root, name, "pipeline")
        
        # --- 1. æ–‡æœ¬åˆ‡ç‰‡é€»è¾‘ (åªå–ç²¾å) ---
        # æ‹¼åˆæ‰€æœ‰æ–‡æœ¬é¡µ
        full_raw_text = ""
        for page in meta['page_structure']:
            if page['type'] == 'text':
                full_raw_text += page['content'] + "\n"
        
        # A. å‰ä¸¤é¡µç²¾å
        front_text = ""
        count = 0
        for p in meta['page_structure']:
            if p['type'] == 'text':
                front_text += p['content'] + "\n"
                count += 1
            if count >= 2: break
        
        # B. æ–¹æ³•è®ºå®šä½ (2000å­—)
        re_method = re.compile(r'^\s*(?:2|3|II|III)\.?\s+(?:Method|Proposed|System|Architecture|Design)', re.I | re.M)
        m_match = re_method.search(full_raw_text)
        method_text = full_raw_text[m_match.start():m_match.start()+2000] if m_match else ""

        # C. ç»“è®ºå®šä½ (References ä¹‹å‰ 2000å­—)
        re_ref = re.compile(r'\n#+\s+(?:References|REFERENCES|Bibliography|å‚è€ƒæ–‡çŒ®)', re.I)
        ref_parts = re_ref.split(full_raw_text)
        pre_ref_text = ref_parts[0]
        # å‰”é™¤ Related Work
        re_related = re.compile(r'\n#+\s+(?:Related Work|RELATED WORK)', re.I)
        conclusion_text = re_related.split(pre_ref_text)[0][-2000:]

        # --- 2. è§†è§‰ç»„ä»¶è§£æ (åˆ©ç”¨åºåˆ—åŒ–åçš„ dict) ---
        visual_md = ""
        if middle_json_dict:
            # æ¸²æŸ“è§†è§‰éƒ¨åˆ†çš„ Markdown (ä»…å«å›¾ç‰‡/è¡¨æ ¼å ä½ç¬¦)
            visual_md = pipeline_union_make(middle_json_dict["pdf_info"], MakeMode.MM_MD, str(Path(local_image_dir).name))

        # --- 3. ç¼åˆæœ€ç»ˆè¾“å‡º (ç”¨äºå–‚ç»™ Qwen) ---
        qwen_prompt = f"# {name}\n\n[FRONT MATTER]\n{front_text[:3000]}\n\n"
        if method_text:
            qwen_prompt += f"[METHODOLOGY SNIPPET]\n{method_text}\n\n"
        qwen_prompt += f"[CONCLUSION SNIPPET]\n{conclusion_text}\n\n"
        qwen_prompt += f"[VISUAL ASSETS]\n{visual_md}"

        # å­˜ç›˜
        with open(Path(local_md_dir) / f"{name}_qwen_input.txt", "w", encoding="utf-8", errors="replace") as f:
            f.write(qwen_prompt)
            
        return True
    except Exception as e:
        logger.error(f"Save error for {name}: {e}")
        return False

# ================= 4. ä¸»å¼•æ“ =================

class EdgeScholarBatchEngine:
    def __init__(self, output_root):
        self.output_root = output_root

    def run_benchmark(self, pdf_folder, batch_size=10):
        abs_folder = os.path.abspath(pdf_folder)
        pdf_paths = [os.path.join(abs_folder, f) for f in os.listdir(abs_folder) if f.lower().endswith(".pdf")][:batch_size]
        
        # 1. Warm-up (ä½¿ç”¨ä½ çš„ sample.pdf é€»è¾‘)
        logger.info("ğŸ”¥ æ­£åœ¨é¢„çƒ­æ¨¡å‹...")
        sample_path = "./input/sample.pdf"
        if os.path.exists(sample_path):
            with open(sample_path, "rb") as f:
                _ = pipeline_doc_analyze([f.read()], ['en'], formula_enable=False, table_enable=False)
        else:
            logger.warning("é¢„çƒ­æ–‡ä»¶ sample.pdf ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ç¯‡è®ºæ–‡é¢„çƒ­")
            _ = pipeline_doc_analyze([open(pdf_paths[0], "rb").read()], ['en'], formula_enable=False, table_enable=False)

        # 2. CPU å¤šæ ¸å‰ªææ‰«æ
        logger.info(f"âš™ï¸ æ­£åœ¨å¹¶è¡Œæ‰«æ {len(pdf_paths)} ç¯‡è®ºæ–‡...")
        t_cpu_start = time.perf_counter()
        with ProcessPoolExecutor(max_workers=min(len(pdf_paths), 10)) as executor:
            meta_list = list(executor.map(cpu_pre_process_worker, pdf_paths))
        valid_meta = [m for m in meta_list if m['status'] == 'success']

        # 3. GPU æ‰¹é‡æ¨ç†
        ocr_needed_data = [m for m in valid_meta if m['ocr_bytes'] is not None]
        serialized_outputs = {} # å­˜æ”¾è„±æ•åçš„å­—å…¸
        
        if ocr_needed_data:
            logger.info(f"ğŸš€ å¯åŠ¨ GPU æ¨ç†ï¼Œå¤„ç† {len(ocr_needed_data)} ç¯‡å«å›¾é¡µ...")
            batch_bytes = [m['ocr_bytes'] for m in ocr_needed_data]
            results = pipeline_doc_analyze(batch_bytes, ['en']*len(batch_bytes), formula_enable=False, table_enable=False)
            
            # --- å…³é”®ï¼šåœ¨ä¸»è¿›ç¨‹å®Œæˆ dict è½¬æ¢ï¼Œè§£å†³ Pickle æŠ¥é”™ ---
            logger.info("âš¡ è½¬æ¢æ•°æ®ç»“æ„ä¸ºå¯åºåˆ—åŒ– Dict...")
            for i, m in enumerate(ocr_needed_data):
                # è¿™ä¸€æ­¥å°† C å¯¹è±¡çš„ infer_result è½¬æ¢ä¸ºçº¯ Python Dict
                local_image_dir, _ = prepare_env(self.output_root, m['name'], "pipeline")
                image_writer = FileBasedDataWriter(local_image_dir)
                
                # è½¬æ¢ä¸º dict (æ³¨æ„ï¼šè¿™é‡Œä¼šäº§ç”Ÿ I/O ä¿å­˜å›¾ç‰‡)
                middle_json = pipeline_result_to_middle_json(
                    results[0][i], results[1][i], results[2][i], 
                    image_writer, "en", True, formula_enabled=False
                )
                serialized_outputs[m['name']] = middle_json

        gpu_time = time.perf_counter() - t_cpu_start
        logger.info(f"âš¡ æ¨ç†ä¸ç»“æ„è½¬æ¢å®Œæˆï¼Œè€—æ—¶: {gpu_time:.2f}s")

        # 4. CPU å¤šæ ¸å¹¶è¡Œåˆ‡ç‰‡ä¸ä¿å­˜ (Markdownç”Ÿæˆ)
        logger.info("ğŸ’¾ å¯åŠ¨å¤šæ ¸å¹¶è¡Œæ–‡æœ¬åˆ‡ç‰‡ä¸ä¿å­˜...")
        t_save_start = time.perf_counter()
        save_tasks = []
        for m in valid_meta:
            m_dict = serialized_outputs.get(m['name'], None)
            save_tasks.append((m_dict, m, self.output_root))

        with ProcessPoolExecutor(max_workers=min(len(save_tasks), 8)) as executor:
            list(executor.map(cpu_save_worker, save_tasks))
            
        save_time = time.perf_counter() - t_save_start
        logger.info(f"âœ… ä¿å­˜è€—æ—¶: {save_time:.2f}s")
        
        total_dur = time.perf_counter() - t_cpu_start
        logger.info(f"ğŸ“Š ç³»ç»Ÿæ€»ååé‡: {60 / (total_dur/len(valid_meta)):.2f} papers/min")

if __name__ == "__main__":
    engine = EdgeScholarBatchEngine("./output/mineru_final_v4")
    engine.run_benchmark("./input/osdi2025", batch_size=10)