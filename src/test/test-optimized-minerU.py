import os
import time
import re
import json
import fitz
import gc
import pickle
from pathlib import Path
from loguru import logger
from concurrent.futures import ProcessPoolExecutor

# ================= 1. ç¯å¢ƒé…ç½® =================
os.environ['MINERU_MODEL_SOURCE'] = "local"
os.environ['MINERU_DEVICE_MODE'] = "cuda:0"
os.environ['MODELSCOPE_LOG_LEVEL'] = '40'
fitz.TOOLS.mupdf_display_errors(False)

from mineru.cli.common import prepare_env
from mineru.data.data_reader_writer import FileBasedDataWriter
from mineru.utils.enum_class import MakeMode
from mineru.backend.pipeline.pipeline_analyze import doc_analyze as pipeline_doc_analyze
from mineru.backend.pipeline.pipeline_middle_json_mkcontent import union_make as pipeline_union_make
from mineru.backend.pipeline.model_json_to_middle_json import result_to_middle_json as pipeline_result_to_middle_json

# ================= 2. å¢å¼ºç‰ˆ CPU é¢„å¤„ç† Worker =================

def cpu_pre_process_worker(pdf_path):
    try:
        # æ›´åŠ ä¸¥æ ¼çš„ç« èŠ‚å®šä½æ­£åˆ™
        # åŒ¹é…ï¼šReferences, Bibliography ç­‰
        re_ref = re.compile(r'\n#?\s*(?:References|REFERENCES|Bibliography|å‚è€ƒæ–‡çŒ®)', re.I)
        # åŒ¹é…ï¼šRelated Work, Background ç­‰
        re_related = re.compile(r'\n#?\s*(?:\d\.?\s+)?(?:Related Work|RELATED WORK|Background|Prior Work)', re.I)
        # åŒ¹é…ï¼šFigure/Table å¼•ç”¨
        re_visual = re.compile(r'\b(Table|Figure|Fig\.)\s+\d+\b', re.I)

        doc = fitz.open(pdf_path)
        total_pages = doc.page_count
        
        page_raw_texts = {}
        ocr_indices = []
        idx_ref = -1
        
        for i in range(total_pages):
            page = doc[i]
            # --- æ ¸å¿ƒï¼šå…¨é‡å¼€å¯ blocks+sort=True ---
            blocks = page.get_text("blocks", sort=True)
            txt = "\n".join([b[4] for b in blocks if b[6] == 0])
            page_raw_texts[i] = txt
            
            # å®šä½ References æ‰€åœ¨é¡µ
            if idx_ref == -1 and i > total_pages * 0.5:
                if re_ref.search(txt):
                    idx_ref = i
            
            # è§†è§‰æ„ŸçŸ¥åˆ¤å®šï¼šåªè¦æœ‰å›¾ç‰‡æˆ– Table/Figure å¼•ç”¨ï¼Œå°±æ ‡è®°ä¸º OCR é¡µ
            if len(page.get_images()) > 0 or re_visual.search(txt):
                # æ’é™¤å‚è€ƒæ–‡çŒ®ä¹‹åçš„é¡µï¼Œå‡å°‘å†—ä½™
                if idx_ref == -1 or i <= idx_ref:
                    ocr_indices.append(i)

        # --- è¯­ä¹‰æå–é€»è¾‘ ---
        # 1. Front Matter: æ— æŸä¿ç•™å‰ä¸¤é¡µå…¨éƒ¨æ–‡æœ¬
        front_text = ""
        for i in range(min(2, total_pages)):
            front_text += page_raw_texts.get(i, "") + "\n"

        # 2. Conclusion: åŠ¨æ€å›æº¯æå–
        # é€»è¾‘ï¼šå– Ref é¡µ + Ref å‰ä¸€é¡µï¼Œç„¶ååˆ‡æ‰ä¸éœ€è¦çš„éƒ¨åˆ†
        conclusion_raw = ""
        if idx_ref != -1:
            # è·å– Ref æ‰€åœ¨é¡µåŠå…¶å‰ä¸€é¡µ
            start_p = max(2, idx_ref - 1) # é¿å¼€å‰ä¸¤é¡µ
            for p in range(start_p, idx_ref + 1):
                conclusion_raw += page_raw_texts.get(p, "") + "\n"
            
            # å‰ªæ A: åˆ‡æ‰ References åŠå…¶ä¹‹åçš„æ‰€æœ‰å†…å®¹
            conclusion_clean = re_ref.split(conclusion_raw)[0]
            # å‰ªæ B: åˆ‡æ‰ Related Work (å¦‚æœåœ¨ç»“è®ºåé¢)
            conclusion_clean = re_related.split(conclusion_clean)[0]
            # ä¿ç•™è¯¥åŒºåŸŸæœ€åçš„ 3500 å­—ç¬¦ï¼ˆé€šå¸¸æ¶µç›–äº†å®Œæ•´çš„ Conclusion å’Œéƒ¨åˆ† Evaluation æ€»ç»“ï¼‰
            conclusion_final = conclusion_clean[-3500:]
        else:
            # æ²¡æ‰¾åˆ° Refï¼Œå–æœ€åä¸¤é¡µ
            conclusion_final = "\n".join([page_raw_texts.get(i, "") for i in range(max(0, total_pages-2), total_pages)])

        # ç”Ÿæˆç”¨äº MinerU å¤„ç†çš„å¾®å‹ PDF (ä»…å«å›¾è¡¨é¡µ)
        pruned_bytes = None
        if ocr_indices:
            new_doc = fitz.open()
            for p in ocr_indices:
                new_doc.insert_pdf(doc, from_page=p, to_page=p)        
            pruned_bytes = new_doc.tobytes(garbage=3, deflate=True)
            new_doc.close()
        
        doc.close()
        return {
            "name": Path(pdf_path).stem,
            "ocr_bytes": pruned_bytes,
            "ocr_mapping": ocr_indices,
            "front_text": front_text,
            "conclusion_text": conclusion_final,
            "status": "success"
        }
    except Exception as e:
        return {"status": "error", "error": str(e), "name": Path(pdf_path).stem}

# ================= 3. CPU ç»“æœä¿å­˜ä¸ç¼åˆ Worker =================

def cpu_save_worker(data_pack):
    """
    å¤šæ ¸å¹¶è¡Œå¤„ç†ï¼šä¿å­˜ä¸ºç²¾ç¾çš„ Markdown æ–‡ä»¶
    """
    (middle_json_dict, meta, output_root) = data_pack
    name = meta['name']
    try:
        local_image_dir, local_md_dir = prepare_env(output_root, name, "pipeline")
        image_writer = FileBasedDataWriter(local_image_dir)
        
        # æ¸²æŸ“ MinerU çš„è§†è§‰è¾“å‡º (è¡¨æ ¼ã€å…¬å¼ã€å›¾ç‰‡å ä½ç¬¦)
        visual_md = ""
        if middle_json_dict:
            visual_md = pipeline_union_make(middle_json_dict["pdf_info"], MakeMode.MM_MD, str(Path(local_image_dir).name))

        # æ„é€  Markdown æŠ¥å‘Š
        report_md = f"""# {name} Analysis Report

## ğŸ“„ [PART 1] Front Matter (Abstract & Intro)
{meta['front_text']}

---

## ğŸ” [PART 2] Visual Evidence (Tables & Figures)
> **Note:** These assets are extracted via MinerU OCR from relevant pages.
{visual_md}

---

## ğŸ [PART 3] Conclusion & Findings
{meta['conclusion_text']}

---
*Generated by EdgeScholar Heterogeneous Pipeline*
"""
        # ä¿å­˜ Markdown æ–‡ä»¶
        save_path = Path(local_md_dir) / f"{name}_report.md"
        with open(save_path, "w", encoding="utf-8", errors="replace") as f:
            f.write(report_md)
            
        return True
    except Exception as e:
        logger.error(f"Save error for {name}: {e}")
        return False

# ================= 4. ä¸»æ‰§è¡Œå¼•æ“ =================

class EdgeScholarBatchEngine:
    def __init__(self, output_root):
        self.output_root = output_root

    def run_benchmark(self, pdf_folder, batch_size=10):
        abs_folder = os.path.abspath(pdf_folder)
        pdf_paths = [os.path.join(abs_folder, f) for f in os.listdir(abs_folder) if f.lower().endswith(".pdf")][:batch_size]
        
        logger.info("ğŸ”¥ åŠ è½½æƒé‡å¹¶é¢„çƒ­...")
        sample_path = "./input/sample.pdf"
        if os.path.exists(sample_path):
            with open(sample_path, "rb") as f:
                _ = pipeline_doc_analyze([f.read()], ['en'], formula_enable=False, table_enable=False)

        # Step 2: å¤šæ ¸æ‰«æä¸ç²¾å‡†å‰ªæ
        logger.info(f"âš™ï¸ æ­£åœ¨æ‰§è¡Œç»“æ„åŒ–æ‰«æ (CPU å¹¶è¡Œ)...")
        t_cpu_start = time.perf_counter()
        with ProcessPoolExecutor(max_workers=min(len(pdf_paths), 10)) as executor:
            meta_list = list(executor.map(cpu_pre_process_worker, pdf_paths))
        valid_meta = [m for m in meta_list if m['status'] == 'success']

        # Step 3: GPU æ‰¹é‡æ¨ç†
        ocr_needed_data = [m for m in valid_meta if m['ocr_bytes'] is not None]
        serialized_outputs = {}
        
        if ocr_needed_data:
            logger.info(f"ğŸš€ å¯åŠ¨ GPU æ¨ç†ï¼Œå¤„ç† {len(ocr_needed_data)} ç¯‡è®ºæ–‡çš„å›¾è¡¨é¡µ...")
            batch_bytes = [m['ocr_bytes'] for m in ocr_needed_data]
            results = pipeline_doc_analyze(batch_bytes, ['en']*len(batch_bytes), formula_enable=False, table_enable=False)
            
            # ä¸»è¿›ç¨‹è¿›è¡Œ dict è½¬æ¢ä»¥è§„é¿ Pickle æŠ¥é”™
            for i, m in enumerate(ocr_needed_data):
                local_image_dir, _ = prepare_env(self.output_root, m['name'], "pipeline")
                image_writer = FileBasedDataWriter(local_image_dir)
                middle_json = pipeline_result_to_middle_json(
                    results[0][i], results[1][i], results[2][i], 
                    image_writer, "en", True, formula_enabled=False
                )
                serialized_outputs[m['name']] = middle_json

        # Step 4: å¤šæ ¸ç»“æœç¼åˆ
        logger.info("ğŸ’¾ æ­£åœ¨å¹¶è¡Œç”Ÿæˆ Markdown æŠ¥å‘Š...")
        save_tasks = []
        for m in valid_meta:
            m_dict = serialized_outputs.get(m['name'], None)
            save_tasks.append((m_dict, m, self.output_root))

        with ProcessPoolExecutor(max_workers=min(len(save_tasks), 8)) as executor:
            list(executor.map(cpu_save_worker, save_tasks))
            
        total_dur = time.perf_counter() - t_cpu_start
        logger.info(f"ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼è¾“å‡ºæ–‡ä»¶å¤¹: {self.output_root}")
        logger.info(f"ğŸ“Š ç³»ç»Ÿæ€»ååé‡: {(total_dur/len(valid_meta)):.2f} seconds/paper")

if __name__ == "__main__":
    engine = EdgeScholarBatchEngine("./output/mineru_final_v5")
    engine.run_benchmark("./input/osdi2025", batch_size=10)