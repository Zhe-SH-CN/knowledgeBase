import os
import time
import re
import json
import fitz
import gc
from pathlib import Path
from loguru import logger
from concurrent.futures import ProcessPoolExecutor

# ================= 1. ç¯å¢ƒé…ç½® =================
os.environ['MINERU_MODEL_SOURCE'] = "local"
os.environ['MINERU_DEVICE_MODE'] = "cuda:0"
os.environ['MODELSCOPE_LOG_LEVEL'] = '40'
fitz.TOOLS.mupdf_display_errors(False)

from mineru.cli.common import prepare_env
from mineru.data.data_reader_writer import FileBasedDataWriter
from mineru.utils.enum_class import MakeMode
from mineru.backend.pipeline.pipeline_analyze import doc_analyze as pipeline_doc_analyze
from mineru.backend.pipeline.pipeline_middle_json_mkcontent import union_make as pipeline_union_make
from mineru.backend.pipeline.model_json_to_middle_json import result_to_middle_json as pipeline_result_to_middle_json

# ================= 2. å¢å¼ºç‰ˆ CPU é¢„å¤„ç† Worker =================

def cpu_pre_process_worker(pdf_path):
    try:
        re_ref = re.compile(r'\n#?\s*(?:References|REFERENCES|Bibliography)', re.I)
        re_visual = re.compile(r'\b(Table|Figure|Fig\.)\s+\d+\b', re.I)

        doc = fitz.open(pdf_path)
        total_pages = doc.page_count
        
        page_raw_texts = {}
        ocr_indices = []
        idx_ref = -1
        
        for i in range(total_pages):
            page = doc[i]
            blocks = page.get_text("blocks", sort=True)
            txt = "\n".join([b[4] for b in blocks if b[6] == 0])
            page_raw_texts[i] = txt
            
            if idx_ref == -1 and i > total_pages * 0.3:
                if re_ref.search(txt): idx_ref = i
            
            if len(page.get_images()) > 0 or re_visual.search(txt):
                if idx_ref == -1 or i <= idx_ref:
                    ocr_indices.append(i)

        # 1. Front Matter: å‰ä¸¤é¡µå…¨é‡æ–‡æœ¬
        front_text = ""
        for i in range(min(2, total_pages)):
            front_text += page_raw_texts.get(i, "") + "\n"

        # 2. ç”Ÿæˆç”¨äºæ¨ç†çš„å¾®å‹ PDF
        pruned_bytes = None
        if ocr_indices:
            new_doc = fitz.open()
            for p in ocr_indices:
                new_doc.insert_pdf(doc, from_page=p, to_page=p)        
            pruned_bytes = new_doc.tobytes(garbage=3, deflate=True)
            new_doc.close()
        
        doc.close()
        return {
            "name": Path(pdf_path).stem,
            "ocr_bytes": pruned_bytes,
            "ocr_mapping": ocr_indices,
            "front_text": front_text,
            "all_texts_dict": page_raw_texts, # ä¼ é€’ç»™ä¿å­˜è¿›ç¨‹ç”¨äºåˆ‡ç‰‡
            "status": "success"
        }
    except Exception as e:
        return {"status": "error", "error": str(e), "name": Path(pdf_path).stem}

# ================= 3. CPU å¤šæ ¸ä¿å­˜ä¸åˆ‡ç‰‡ Worker (v3.4) =================

def cpu_save_worker(data_pack):
    """
    è´Ÿè´£ï¼š
    1. æ–‡æœ¬ç²¾å‡†åˆ‡ç‰‡ (Before Abstract / Abstract / Intro)
    2. è§†è§‰ç»„ä»¶é¡ºåºé‡å‘½å (å’Œè®ºæ–‡å‡ºç°é¡ºåºä¸€è‡´)
    3. ç»“è®ºæå– (ä» all_texts_dict åŠ¨æ€è®¡ç®—)
    """
    (middle_json_dict, meta, output_root) = data_pack
    name = meta['name']
    
    try:
        paper_folder = Path(output_root) / name
        img_folder = paper_folder / "images"
        
        # --- A. è§†è§‰ç»„ä»¶é¡ºåºå¤„ç† ---
# --- B. è§†è§‰ç»„ä»¶å¤„ç† (ä¿®å¤è·¯å¾„æ‹¼æ¥ä¸é€’å½’æœç´¢) ---
        visual_md = ""
        if middle_json_dict:
            img_idx = 0
            # è®°å½• {æ—§æ–‡ä»¶å(hash): æ–°æ–‡ä»¶å(index)}ï¼Œé˜²æ­¢å¤šå¤„å¼•ç”¨åŒä¸€å¼ å›¾æ—¶é‡å¤é‡å‘½å
            renamed_map = {} 
            
            # 1. å®šä¹‰é€’å½’æŸ¥æ‰¾å™¨ï¼šæ‰¾å‡ºæ‰€æœ‰å«å›¾ç‰‡çš„ block
            def get_visual_blocks(obj):
                found = []
                if isinstance(obj, dict):
                    # åˆ¤æ–­æ˜¯å¦æ˜¯è§†è§‰å—
                    if obj.get("img_path") or obj.get("table_img_path"):
                        found.append(obj)
                    # é€’å½’æŸ¥æ‰¾å­å…ƒç´ 
                    for k, v in obj.items():
                        found.extend(get_visual_blocks(v))
                elif isinstance(obj, list):
                    for item in obj:
                        found.extend(get_visual_blocks(item))
                return found

            # è·å–æ‰€æœ‰è§†è§‰å—
            all_visual_blocks = get_visual_blocks(middle_json_dict)

            # 2. å¤„ç†å›¾ç‰‡é‡å‘½åä¸ MD ç”Ÿæˆ
            for block in all_visual_blocks:
                # è·å–åŸå§‹è·¯å¾„ (å¯èƒ½æ˜¯ "images/xxx_hash.jpg" æˆ– "xxx_hash.jpg")
                raw_rel_path = block.get("img_path") or block.get("table_img_path")
                if not raw_rel_path: continue

                # ã€å…³é”®ä¿®å¤ã€‘ï¼šåªæå–æ–‡ä»¶åï¼Œå¿½ç•¥ JSON é‡Œçš„ç›®å½•å‰ç¼€
                hash_filename = Path(raw_rel_path).name 
                
                # æ„é€ ç‰©ç†è·¯å¾„
                old_file_path = img_folder / hash_filename
                
                # ç¡®å®šæ–°æ–‡ä»¶å
                if hash_filename in renamed_map:
                    # å¦‚æœå·²ç»é‡å‘½åè¿‡ï¼ˆåŒä¸€å¼ å›¾è¢«å¤šæ¬¡å¼•ç”¨ï¼‰ï¼Œç›´æ¥å¤ç”¨
                    final_name = renamed_map[hash_filename]
                else:
                    # å¦‚æœæ˜¯æ–°å›¾ï¼Œç”Ÿæˆæ–°åå­—
                    if old_file_path.exists():
                        ext = old_file_path.suffix
                        new_name = f"{name}-{img_idx}{ext}"
                        new_file_path = img_folder / new_name
                        
                        try:
                            os.rename(old_file_path, new_file_path)
                            # è®°å½•æ˜ å°„å…³ç³»
                            renamed_map[hash_filename] = new_name
                            final_name = new_name
                            img_idx += 1
                        except OSError:
                            # å¦‚æœé‡å‘½åå¤±è´¥ï¼ˆæå°‘è§ï¼‰ï¼Œæ²¿ç”¨æ—§å
                            final_name = hash_filename
                    else:
                        # å›¾ç‰‡æ–‡ä»¶ç‰©ç†ä¸¢å¤±ï¼Œè·³è¿‡ç”Ÿæˆ MD
                        # logger.warning(f"Image missing: {old_file_path}")
                        continue

                # 3. ç”Ÿæˆ Markdown
                # åŒºåˆ†è¡¨æ ¼å’Œå›¾ç‰‡
                block_type = block.get("type", "").lower()
                tag = "ğŸ“Š Table" if "table" in block_type else "ğŸ–¼ï¸ Figure"
                caption = block.get("caption", "").strip()
                
                visual_md += f"### {tag} {img_idx} (Source: Page {block.get('page_idx', '?')})\n"
                if caption:
                    visual_md += f"> **Caption:** {caption}\n\n"
                
                # å†™å…¥å›¾ç‰‡é“¾æ¥
                visual_md += f"![](images/{final_name})\n\n"
        # --- B. æ–‡æœ¬åˆ‡ç‰‡é€»è¾‘ (åŒ…å«ä¹‹å‰ç¼ºå¤±çš„ conclusion_text è®¡ç®—) ---
        # 1. æ‹¼åˆå…¨æ–‡ç”¨äºæœç´¢
        all_pages_indices = sorted(meta['all_texts_dict'].keys())
        full_raw_text = "\n".join([meta['all_texts_dict'][i] for i in all_pages_indices])
        
        # 2. å‰éƒ¨åˆ‡ç‰‡ (Metadata/Abstract/Intro)
        abs_regex = re.compile(r'(Abstract|ABSTRACT)', re.M)
        intro_regex = re.compile(r'\n\s*(?:1\.?\s+)?(Introduction|INTRODUCTION)', re.I | re.M)
        
        abs_m = abs_regex.search(full_raw_text)
        intro_m = intro_regex.search(full_raw_text)
        
        metadata_part = full_raw_text[:abs_m.start()].strip() if abs_m else "Not Found"
        
        if abs_m and intro_m:
            abstract_part = full_raw_text[abs_m.start():intro_m.start()].strip()
            # Introduction å–ä»æ ‡é¢˜å¼€å§‹åˆ°åç»­ 3000 å­—ç¬¦ï¼ˆé˜²æ­¢å¤ªé•¿ï¼‰
            introduction_part = full_raw_text[intro_m.start():intro_m.start()+4000].strip()
        else:
            abstract_part = "Not Found"
            introduction_part = "Not Found"

        # 3. ç»“è®ºåˆ‡ç‰‡ (ä» all_texts_dict åŠ¨æ€æå–å¹¶å‰”é™¤ Related Work)
        re_conc = re.compile(r'\n#?\s*(?:\d\.?\s+)?(?:Conclusion|CONCLUSION|Summary)', re.I | re.M)
        re_ref = re.compile(r'\n#?\s*(?:References|REFERENCES|Bibliography|å‚è€ƒæ–‡çŒ®)', re.I | re.M)
        re_related = re.compile(r'\n#?\s*(?:\d\.?\s+)?(?:Related Work|RELATED WORK)', re.I | re.M)

        conclusion_final = "Conclusion not identified."
        conc_m = re_conc.search(full_raw_text)
        if conc_m:
            # ä»ç»“è®ºå¼€å§‹ï¼Œå¾€åæ‰¾å‚è€ƒæ–‡çŒ®
            post_conc_text = full_raw_text[conc_m.start():]
            # å…ˆåˆ‡æ‰å‚è€ƒæ–‡çŒ®
            pre_ref_text = re_ref.split(post_conc_text)[0]
            # å†åˆ‡æ‰å¯èƒ½å­˜åœ¨çš„ Related Work (å¦‚æœå®ƒåœ¨ç»“è®ºä¹‹å)
            clean_conc = re_related.split(pre_ref_text)[0]
            conclusion_final = clean_conc.strip()
        else:
            # å…œåº•ï¼šå¦‚æœæ²¡æ‰¾åˆ°ç»“è®ºæ ‡é¢˜ï¼Œå–å…¨æ–‡æœ€å 1500 å­—ç¬¦ï¼ˆå‰”é™¤å‚è€ƒæ–‡çŒ®åï¼‰
            pre_ref_text = re_ref.split(full_raw_text)[0]
            conclusion_final = pre_ref_text[-1500:].strip()

        # --- C. ç¼åˆæœ€ç»ˆæŠ¥å‘Š ---
        final_md = f"""# Paper: {name}

## 1. Metadata (Before Abstract)
{metadata_part}

## 2. Abstract
{abstract_part}

## 3. Introduction
{introduction_part}

## 4. Methodology
(Methodology section is skipped)

## 5. Conclusion & Findings
{conclusion_final}

## 6. Visual Components (In Appearance Order)
{visual_md if visual_md else "No tables or figures found."}

---
*Generated by EdgeScholar Optimized MD v3.8.1*
"""
        with open(paper_folder / f"{name}_report.md", "w", encoding="utf-8", errors="replace") as f:
            f.write(final_md)
            
        return True
    except Exception as e:
        logger.error(f"Save error for {name}: {e}")
        return False
# ================= 4. ä¸»æ‰§è¡Œå¼•æ“ =================

class EdgeScholarBatchEngine:
    def __init__(self, output_root):
        self.output_root = output_root

    def run_benchmark(self, pdf_folder, batch_size=10):
        abs_folder = os.path.abspath(pdf_folder)
        pdf_paths = [os.path.join(abs_folder, f) for f in os.listdir(abs_folder) if f.lower().endswith(".pdf")][:batch_size]
        
        logger.info("ğŸ”¥ é¢„çƒ­æ˜¾å¡èµ„æº...")
        sample_path = "./input/sample.pdf"
        if os.path.exists(sample_path):
            _ = pipeline_doc_analyze([open(sample_path, "rb").read()], ['en'], formula_enable=False)

        # Step 2: CPU å¹¶è¡Œæ‰«æ
        t_start = time.perf_counter()
        with ProcessPoolExecutor(max_workers=min(len(pdf_paths), 10)) as executor:
            meta_list = list(executor.map(cpu_pre_process_worker, pdf_paths))
        valid_meta = [m for m in meta_list if m['status'] == 'success']

        # Step 3: GPU æ‰¹é‡æ¨ç†
        ocr_needed_data = [m for m in valid_meta if m['ocr_bytes'] is not None]
        serializable_results = {}
        
        if ocr_needed_data:
            logger.info(f"ğŸš€ GPU æ¨ç†: {len(ocr_needed_data)} ç¯‡å«å›¾è®ºæ–‡...")
            batch_bytes = [m['ocr_bytes'] for m in ocr_needed_data]
            results = pipeline_doc_analyze(batch_bytes, ['en']*len(batch_bytes), formula_enable=False, table_enable=False)
            
            # --- å…³é”®ä¿®å¤ï¼šåœ¨ä¸»è¿›ç¨‹è½¬ä¸ºçº¯ Dictï¼Œè§£å†³ ctypes åºåˆ—åŒ–é—®é¢˜ ---
            logger.info("âš¡ è½¬æ¢ C å¯¹è±¡ä¸ºå¯åºåˆ—åŒ– Dict...")
            for i, m in enumerate(ocr_needed_data):
                paper_img_dir = Path(self.output_root) / m['name'] / "images"
                os.makedirs(paper_img_dir, exist_ok=True)
                
                # è¿™ä¸€æ­¥ä¼šå°† results é‡Œçš„æŒ‡é’ˆè§£æ„æˆå¯åºåˆ—åŒ–çš„å­—å…¸æ•°æ®
                # æ³¨æ„ï¼šä¸ºäº†è·å–å®Œæ•´çš„ middle_jsonï¼Œå¿…é¡»ä¼ å…¥ image_writer
                image_writer = FileBasedDataWriter(str(paper_img_dir))
                middle_json_dict = pipeline_result_to_middle_json(
                    results[0][i], results[1][i], results[2][i], 
                    image_writer, "en", True, formula_enabled=False
                )
                serializable_results[m['name']] = middle_json_dict

        # Step 4: å¤šæ ¸å¹¶è¡Œä¿å­˜
        logger.info("ğŸ’¾ å¤šæ ¸å¹¶è¡Œä¿å­˜ v3.6 (å›¾ç‰‡é‡å‘½å + æ–‡æœ¬æ·±åº¦åˆ‡ç‰‡)...")
        save_tasks = []
        for m in valid_meta:
            res_dict = serializable_results.get(m['name'], None)
            save_tasks.append((res_dict, m, self.output_root))

        with ProcessPoolExecutor(max_workers=min(len(save_tasks), 8)) as executor:
            list(executor.map(cpu_save_worker, save_tasks))
            
        logger.info(f"ğŸ“Š å¹³å‡è€—æ—¶: {((time.perf_counter()-t_start)/len(valid_meta)):.2f} seconds/paper")

if __name__ == "__main__":
    # æ›´æ–°ç›®å½•åä¸º v3.6
    engine = EdgeScholarBatchEngine("./output/mineru_batch_v3.6")
    engine.run_benchmark("./input/osdi2025", batch_size=10)