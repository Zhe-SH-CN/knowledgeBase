import os
import time
import json
import gc  # å¼•å…¥åƒåœ¾å›æ”¶
from pathlib import Path
from loguru import logger
from concurrent.futures import ThreadPoolExecutor

# ================= é…ç½®åŒº =================
os.environ['MINERU_MODEL_SOURCE'] = "modelscope"
os.environ['MINERU_DEVICE_MODE'] = "cuda:0" 

# æ‰¹å¤„ç†å¤§å°ï¼šæ ¹æ®æ˜¾å­˜å’Œå†…å­˜è°ƒæ•´ã€‚3090 (24G) å»ºè®®è®¾ä¸º 5-8
# è®¾ä¸º 5 æ„å‘³ç€æ¯æ¬¡å¹¶è¡Œå¤„ç† 5 ç¯‡è®ºæ–‡ï¼Œå¤„ç†å®Œé‡Šæ”¾å†…å­˜ï¼Œå†å¤„ç†ä¸‹ 5 ç¯‡
BATCH_SIZE = 5 

from mineru.cli.common import prepare_env
from mineru.data.data_reader_writer import FileBasedDataWriter
from mineru.utils.enum_class import MakeMode
from mineru.backend.pipeline.pipeline_analyze import doc_analyze as pipeline_doc_analyze
from mineru.backend.pipeline.pipeline_middle_json_mkcontent import union_make as pipeline_union_make
from mineru.backend.pipeline.model_json_to_middle_json import result_to_middle_json as pipeline_result_to_middle_json

def read_raw_pdf(pdf_path):
    """è¯»å–åŸå§‹äºŒè¿›åˆ¶æ•°æ®"""
    try:
        with open(pdf_path, "rb") as f:
            raw_bytes = f.read()
        return pdf_path, raw_bytes
    except Exception as e:
        logger.error(f"è¯»å–å¤±è´¥ {pdf_path}: {e}")
        return None, None

def save_result(pack):
    """ä¿å­˜ç»“æœé€»è¾‘ (ä¿®å¤å‚æ•°å)"""
    idx, res, imgs, doc, lang, ocr_en, pdf_path, output_dir = pack
    file_name = pdf_path.stem
    
    local_image_dir, local_md_dir = prepare_env(output_dir, file_name, "pipeline")
    image_writer = FileBasedDataWriter(local_image_dir)
    md_writer = FileBasedDataWriter(local_md_dir)

    # ã€ä¿®å¤ã€‘è¿™é‡Œå‚æ•°ååº”ä¸º formula_enabled (è¿‡å»ç‰ˆæœ¬å¯èƒ½æ˜¯ formula_enable)
    # å¦‚æœæŠ¥é”™ï¼Œè¯·å°è¯•æ£€æŸ¥ mineru ç‰ˆæœ¬ï¼Œæ–°ç‰ˆé€šå¸¸æ˜¯ formula_enabled
    middle_json = pipeline_result_to_middle_json(
        res, imgs, doc, image_writer, lang, ocr_en, 
        formula_enabled=False # <--- å·²ä¿®æ­£
    )

    image_relative_dir = str(Path(local_image_dir).name)
    md_content = pipeline_union_make(middle_json["pdf_info"], MakeMode.MM_MD, image_relative_dir)

    md_file = Path(local_md_dir) / f"{file_name}.md"
    md_writer.write_string(md_file.name, md_content)

def batch_process(input_dir, output_dir):
    input_path = Path(input_dir)
    pdf_files = list(input_path.glob("*.pdf"))
    
    if not pdf_files:
        logger.error("æœªæ‰¾åˆ° PDF æ–‡ä»¶")
        return

    logger.info(f"ğŸ“‚ å‘ç° {len(pdf_files)} ç¯‡ PDFï¼Œå‡†å¤‡å¤„ç†...")

    # 1. å¿«é€Ÿè¯»å–æ‰€æœ‰æ–‡ä»¶è¿›å†…å­˜ (PDFæ–‡ä»¶æœ¬èº«ä¸å¤§ï¼Œå¯ä»¥å…¨éƒ¨è¯»å…¥)
    t_load_start = time.time()
    valid_pdfs = []
    raw_bytes_list = []
    
    with ThreadPoolExecutor(max_workers=16) as executor:
        results = executor.map(read_raw_pdf, pdf_files)
        for p_path, p_bytes in results:
            if p_bytes:
                valid_pdfs.append(p_path)
                raw_bytes_list.append(p_bytes)
    
    logger.info(f"âœ… è¯»å–å®Œæˆ {len(valid_pdfs)} ç¯‡ï¼Œè€—æ—¶: {time.time()-t_load_start:.2f}s")
    if not valid_pdfs: return

    # 2. æ¨¡å‹é¢„çƒ­ (Warm-up)
    # å–ç¬¬ä¸€ç¯‡å•ç‹¬è·‘ï¼Œåˆå§‹åŒ– CUDA Context
    logger.info("ğŸ”¥ [Warmup] æ­£åœ¨è¿›è¡Œæ¨¡å‹é¢„çƒ­...")
    t_warmup_start = time.time()
    _ = pipeline_doc_analyze(
        [raw_bytes_list[0]], ['en'], 
        parse_method="auto", formula_enable=False, table_enable=False
    )
    logger.info(f"ğŸ”¥ é¢„çƒ­å®Œæˆï¼Œè€—æ—¶: {time.time()-t_warmup_start:.2f}s")

    # 3. Mini-Batch å¾ªç¯æ¨ç† (æ ¸å¿ƒä¼˜åŒ–)
    # è·³è¿‡é¢„çƒ­çš„é‚£ä¸€ç¯‡ï¼Œå¤„ç†å‰©ä¸‹çš„
    remaining_pdfs = valid_pdfs[1:]
    remaining_bytes = raw_bytes_list[1:]
    total_remaining = len(remaining_pdfs)

    if total_remaining == 0:
        logger.info("æ²¡æœ‰æ›´å¤šæ–‡ä»¶éœ€è¦å¤„ç†")
        return

    logger.info(f"ğŸš€ å¼€å§‹åˆ†æ‰¹å¤„ç†å‰©ä½™ {total_remaining} ç¯‡ (Batch Size: {BATCH_SIZE})...")
    
    # å¾ªç¯åˆ‡ç‰‡
    for i in range(0, total_remaining, BATCH_SIZE):
        batch_pdfs = remaining_pdfs[i : i + BATCH_SIZE]
        batch_bytes = remaining_bytes[i : i + BATCH_SIZE]
        current_batch_num = (i // BATCH_SIZE) + 1
        
        logger.info(f"âš¡ [Batch {current_batch_num}] å¤„ç† {len(batch_pdfs)} ç¯‡...")
        t_batch_start = time.time()

        try:
            # --- æ¨ç† ---
            results_pack = pipeline_doc_analyze(
                batch_bytes, 
                ['en'] * len(batch_bytes), 
                parse_method="auto", 
                formula_enable=False, 
                table_enable=False
            )
            
            # --- ä¿å­˜ ---
            # è§£åŒ…ç»“æœ
            infer_results, all_images, all_docs, langs, ocrs = results_pack
            
            for idx, pdf_path in enumerate(batch_pdfs):
                pack = (idx, infer_results[idx], all_images[idx], all_docs[idx], langs[idx], ocrs[idx], pdf_path, output_dir)
                save_result(pack)

            batch_time = time.time() - t_batch_start
            logger.info(f"âœ… [Batch {current_batch_num}] å®Œæˆï¼Œè€—æ—¶: {batch_time:.2f}s (Avg: {batch_time/len(batch_pdfs):.2f}s/ç¯‡)")

        except Exception as e:
            logger.error(f"âŒ [Batch {current_batch_num}] å¤„ç†å¤±è´¥: {e}")
        
        # --- å†…å­˜æ¸…ç† ---
        # æ˜¾å¼åˆ é™¤å¼•ç”¨ï¼Œå¹¶å¼ºåˆ¶ GCï¼Œé˜²æ­¢å›¾ç‰‡æ•°æ®åœ¨å†…å­˜å †ç§¯
        del batch_bytes
        del results_pack
        gc.collect() 

    logger.info(f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å…¨éƒ¨å®Œæˆã€‚è¾“å‡ºç›®å½•: {output_dir}")

if __name__ == "__main__":
    in_dir = "./osdi2025" 
    out_dir = "./mineru_batch_output"
    
    if os.path.exists(in_dir):
        batch_process(in_dir, out_dir)
    else:
        logger.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {in_dir}")